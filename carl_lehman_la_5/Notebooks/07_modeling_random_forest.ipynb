{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will fit a random forest model on TFIDF tokenized data, count vectorized data, and count vectorized data with sentiment analysis. This is my last attempt at creating a model that can accurately (accuracy score greater than 75 percent) predict which subreddit a post came from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in my train, test splitted dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('../Data/reddit.csv', index_col=0)\n",
    "X_train = pd.read_csv('../Data/X_train.csv', header = None, index_col=0)\n",
    "X_test = pd.read_csv('../Data/X_test.csv', header = None, index_col=0)\n",
    "y_train = pd.read_csv('../Data/y_train.csv', header = None, index_col=0)\n",
    "y_test = pd.read_csv('../Data/y_test.csv', header = None, index_col=0)\n",
    "X_train_sen = pd.read_csv('../Data/X_train_sen.csv', index_col=0)\n",
    "X_test_sen = pd.read_csv('../Data/X_test_sen.csv', index_col=0)\n",
    "y_train_sen = pd.read_csv('../Data/y_train_sen.csv', header = None, index_col=0)\n",
    "y_test_sen = pd.read_csv('../Data/y_test_sen.csv', header = None, index_col=0)\n",
    "reddit.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom list of stop words that includes numbers and common terms found in urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "custom_stopwords.extend(['10', '12', '13', '14', '15', '18', '25','200', '000','https', 'com', 'youtube', 'www'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest on TFIDF data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extensive grid searching, the best parameters I found are as follows:\n",
    "\n",
    "tfidf_min_df: Ignore terms that don't appear in at least 1 document\n",
    "\n",
    "tfidf_max_df: Ignore terms that appear in over $85 \\%$ of the documents\n",
    "\n",
    "tfidf_norm: Regularize the data with an l2 norm\n",
    "\n",
    "rf_n_estimators: Create 100 decision trees for voting\n",
    "\n",
    "rf_max_features: The max features for each decision tree is determined by a log2 scale\n",
    "\n",
    "rf_min_samples_split: At least 3 samples to split an internal node\n",
    "\n",
    "rf_min_samples_leaf: At least 1 sample to be at the leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__min_df': [1, 3], 'tfidf__max_df': [0.8, 0.85], 'tfidf__norm': ['l1', 'l2'], 'rf__n_estimators': [90, 100], 'rf__max_features': ['auto', 'log2', 'sqrt'], 'rf__min_samples_split': [3, 4], 'rf__min_samples_leaf': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=custom_stopwords)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfidf__min_df': [1,3],\n",
    "    'tfidf__max_df': [.80,.85],\n",
    "    'tfidf__norm': ['l1','l2'],\n",
    "    'rf__n_estimators': [90,100],\n",
    "    'rf__max_features': ['auto', 'log2', 'sqrt'],\n",
    "    'rf__min_samples_split': [3,4],\n",
    "    'rf__min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params)\n",
    "\n",
    "gs.fit(X_train[1], y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9906103286384976"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train[1],y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6197183098591549"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test[1],y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_features': 'log2',\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 3,\n",
       " 'rf__n_estimators': 100,\n",
       " 'tfidf__max_df': 0.85,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__norm': 'l2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first random forest model scored an accuracy of $61.9\\%$ and seems to be severely overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest on count vectorized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll see if a random forest model performs better on data that is represented as a count rather than a ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=custom_stopwords)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extensive grid searching, the best parameters I found are as follows:\n",
    "\n",
    "cvec_min_df: Ignore words that don't appear in at least 1 document\n",
    "\n",
    "cvec_max_df: Ignore words that appear in over $90\\%$ of the documents\n",
    "\n",
    "rf_n_estimators: Make 100 decision trees for voting\n",
    "\n",
    "rf_max_features: Scale the max features for each decision tree by a square root\n",
    "\n",
    "rf_min_samples_split: There needs to be at least 3 samples to split an internal node\n",
    "\n",
    "rf_min_samples_leaf: There needs to be at least 2 samples at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'cvec__min_df': [1,3],\n",
    "    'cvec__max_df': [.85,.9],\n",
    "    'rf__n_estimators': [90,100],\n",
    "    'rf__max_features': ['auto', 'log2', 'sqrt'],\n",
    "    'rf__min_samples_split': [3,4],\n",
    "    'rf__min_samples_leaf': [1,2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cvec = GridSearchCV(pipe, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['ie', 'mor...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'cvec__min_df': [1, 3], 'cvec__max_df': [0.85, 0.9], 'rf__n_estimators': [90, 100], 'rf__max_features': ['auto', 'log2', 'sqrt'], 'rf__min_samples_split': [3, 4], 'rf__min_samples_leaf': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec.fit(X_train[1], y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835680751173709"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec.score(X_train[1],y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6591549295774648"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec.score(X_test[1],y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__min_df': 1,\n",
       " 'rf__max_features': 'sqrt',\n",
       " 'rf__min_samples_leaf': 2,\n",
       " 'rf__min_samples_split': 3,\n",
       " 'rf__n_estimators': 100}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cvec.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My random forest model performed better on count vectorized data, with an accuracy of $65.9 \\%$. It is also less overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest on count vectorized data with sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if adding features for sentiment analysis will help our random forest model differentiate between the subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extensive grid searching, the best parameters I found are as follows:\n",
    "\n",
    "n_estimators: Create 90 decision trees for voting\n",
    "\n",
    "max_feature: Each decision tree has a max feature size of the log2 of the total features\n",
    "\n",
    "min_samples_split: There needs to be at least 4 samples to split an internal node\n",
    "\n",
    "min_samples_leaf: There needs to be at least 1 sample at a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [80,90],\n",
    "    'max_features': ['auto', 'log2', 'sqrt'],\n",
    "    'min_samples_split': [3,4],\n",
    "    'min_samples_leaf': [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(RandomForestClassifier(), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [90, 100], 'max_features': ['auto', 'log2', 'sqrt'], 'min_samples_split': [3, 4], 'min_samples_leaf': [1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train_sen,y_train_sen[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586854460093897"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train_sen,y_train_sen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5915492957746479"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_sen,y_test_sen[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 'log2',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 4,\n",
       " 'n_estimators': 90}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model did not improve with an accuracy of $59.1 \\%$, thus sentiment analysis did not help with regards to differentiating between the subreddits. Let's look at the most important features from my random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(data = [gs.best_estimator_.feature_importances_], columns = X_train_sen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.abs().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.072381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.057525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.052037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinton</th>\n",
       "      <td>0.020468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>0.015775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "neutral   0.072381\n",
       "negative  0.057525\n",
       "positive  0.052037\n",
       "clinton   0.020468\n",
       "trump     0.015775"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sort_values(ascending=False, by = 0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that in this model the sentiments are the most important features, but this model only scored $9 \\%$ better than a coin flip, leaving me to conclude that they are not very good features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest performed the best out of all of my models with its best accuracy score being $65.9 \\%$ on a count vectorized dataframe. However, this score is only marginal better than my other two models, logistic regression and k-nearest neighbors, which had accuracy scores of $63 \\%$ and $60 \\%$ respectively. In general, it was hard to differentiate between the two subreddits, r/democrat, and r/republican, leaving me to conclude that the topics discuss on those two subreddits are laregely the same. I suspect this is becuase a large majority of posts are news headlines, and are thus have the same title post on both of the subreddits. This is a good thing, becuase this implies that neither party has a bias or preference as to what news or topics are to be discussed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
